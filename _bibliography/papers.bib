---
---

@INPROCEEDINGS{embc,
  author={Chitnis, Soham and Liu, Sidong and Dash, Tirtharaj and Verlekar, Tanmay Tulsidas and Di Ieva, Antonio and Berkovsky, Shlomo and Vig, Lovekesh and Srinivasan, Ashwin},
  booktitle={2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)}, 
  title={Domain-Specific Pre-training Improves Confidence in Whole Slide Image Classification}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  doi={10.1109/EMBC40787.2023.10340659},
  html={https://ieeexplore.ieee.org/document/10340659},
  abbr={EMBC},
  code={https://github.com/soham-chitnis10/WSI-domain-specific/},
  slides={https://drive.google.com/file/d/1Vx3M6zME7QIETaiFsLp8aafz5Oz0uORE/view?usp=sharing}}

  @misc{chitnis2023spacnnldvae,
      title={SpACNN-LDVAE: Spatial Attention Convolutional Latent Dirichlet Variational Autoencoder for Hyperspectral Pixel Unmixing}, 
      author={Soham Chitnis and Kiran Mantripragada and Faisal Z. Qureshi},
      year={2024},
      eprint={2311.10701},
      abbr={IGARSS},
      arxiv={2311.10701}, 
      code={https://github.com/faisalqureshi/cnn-ldvae}
}

@inproceedings{chitnis-etal-2024-tt,
    title = "AutoRef: Generating Refinements of Reviews Given Guidelines",
    author = "Chitnis, Soham  and
      Patwardhan, Manasi  and
      Srinivasan, Ashwin  and
      Verlekar, Tanmay Tulsidas  and
      Vig, Lovekesh  and
      Shroff, Gautam",
    editor = "Ghosal, Tirthankar  and
      Singh, Amanpreet  and
      Waard, Anita  and
      Mayr, Philipp  and
      Naik, Aakanksha  and
      Weller, Orion  and
      Lee, Yoonjoo  and
      Shen, Shannon  and
      Qin, Yanxia",
    booktitle = "Proceedings of the Fourth Workshop on Scholarly Document Processing (SDP 2024)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.sdp-1.17",
    pages = "175--190",
    abbr ={ACL},
    abstract = "When examining reviews of research papers, we can distinguish between two hypothetical referees: the maximally lenient referee who accepts any paper with a vacuous review and the maximally strict one who rejects any paper with an overly pedantic review. Clearly, both are of no practical value. Our interest is in a referee who makes a balanced judgement and provides a review abiding by the guidelines. In this paper, we present a case study of automatic correction of an existing machine-generated or human review. The ${\tt{AutoRef}\ }$ system implements an iterative approach that progressively {``}refines{''} a review by attempting to make it more compliant with pre-defined requirements of a {``}good{''} review. It implements the following steps: (1) Translate the review requirements into a specification in natural language, of {``}yes/no{''} questions; (2) Given a $(paper,review)$ pair, extract answers to the questions; (3) Use the results in (2) to generate a new review; and (4) Return to Step (2) with the paper and the new review. Here, (2) and (3) are implemented by large language model (LLM) based agents. We present a case study using papers and reviews made available for the International Conference on Learning Representations (ICLR). Our initial empirical results suggest that ${\tt{AutoRef}\ }$ progressively improves the compliance of the generated reviews to the specification. Currently designed specification makes ${\tt{AutoRef}\ }$ progressively generate reviews which are stricter, making the decisions more inclined towards {``}rejections{''}. This demonstrates the applicability of {\$}AutoRef {\$} for: (1) The progressive correction of overly lenient reviews, being useful for referees and meta-reviewers; and (2) The generation of progressively stricter reviews for a paper, starting from a vacuous review ({``}Great paper. Accept.{''}), facilitating authors when trying to assess weaknesses in their papers.",
}